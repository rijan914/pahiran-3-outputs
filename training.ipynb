{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"/Users/rijanbhandari/COLLEGE/sixthsem/minorproject/Pahiran/RESTART/training_module.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import ResNet101\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from tensorflow.keras.layers.experimental.preprocessing import StringLookup\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "import matplotlib.image as mpimg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Topwear', 'Bottomwear', 'Footwear'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "styles = get_df()\n",
    "\n",
    "styles[\"subCategory\"].unique() # we can check by this code that we only have three subcategory now.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Bottomwear', 'Footwear', 'Topwear'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "le = LabelEncoder()\n",
    "#\n",
    "styles[\"subCategory\"] = le.fit_transform(styles[\"subCategory\"])\n",
    "\n",
    "styles.head()\n",
    "\n",
    "le.classes_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error reading image '/Users/rijanbhandari/COLLEGE/sixthsem/minorproject/Pahiran/Miniversion/smalldataset/images/28492.jpg': Invalid shape\n",
      "Error reading image '/Users/rijanbhandari/COLLEGE/sixthsem/minorproject/Pahiran/Miniversion/smalldataset/images/14776.jpg': Invalid shape\n",
      "Error reading image '/Users/rijanbhandari/COLLEGE/sixthsem/minorproject/Pahiran/Miniversion/smalldataset/images/5408.jpg': Invalid shape\n",
      "Error reading image '/Users/rijanbhandari/COLLEGE/sixthsem/minorproject/Pahiran/Miniversion/smalldataset/images/1799.jpg': Invalid shape\n",
      "Error reading image '/Users/rijanbhandari/COLLEGE/sixthsem/minorproject/Pahiran/Miniversion/smalldataset/images/2311.jpg': Invalid shape\n",
      "Error reading image '/Users/rijanbhandari/COLLEGE/sixthsem/minorproject/Pahiran/Miniversion/smalldataset/images/10284.jpg': Invalid shape\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " images (InputLayer)         [(None, 80, 60, 3)]       0         \n",
      "                                                                 \n",
      " resnet101 (Functional)      (None, 3, 2, 2048)        42658176  \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 2, 1, 32)          262176    \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1024)              66560     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 512)               524800    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 3)                 195       \n",
      "                                                                 \n",
      " subCategory (Activation)    (None, 3)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 43684387 (166.64 MB)\n",
      "Trainable params: 1026211 (3.91 MB)\n",
      "Non-trainable params: 42658176 (162.73 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# styles, articleTypeLB, genderLB, baseColourLB, seasonLB, usageLB = my_le(styles)\n",
    "\n",
    "sub_train,sub_val,sub_test=make_input_xx(make_input_array_subcate(styles))\n",
    "sub_model = building_model(80,60)\n",
    "sub_model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rijanbhandari/COLLEGE/sixthsem/minorproject/Pahiran/environment/lib/python3.10/site-packages/keras/src/backend.py:5727: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 204s 101ms/step - loss: 0.2008 - accuracy: 0.9605 - val_loss: 0.1341 - val_accuracy: 0.9778\n",
      "Epoch 2/6\n",
      "2000/2000 [==============================] - 204s 102ms/step - loss: 0.1651 - accuracy: 0.9815 - val_loss: 0.1171 - val_accuracy: 0.9839\n",
      "Epoch 3/6\n",
      "2000/2000 [==============================] - 199s 100ms/step - loss: 0.0933 - accuracy: 0.9818 - val_loss: 0.0609 - val_accuracy: 0.9894\n",
      "Epoch 4/6\n",
      "2000/2000 [==============================] - 199s 100ms/step - loss: 0.1181 - accuracy: 0.9883 - val_loss: 0.0827 - val_accuracy: 0.9890\n",
      "Epoch 5/6\n",
      " 178/2000 [=>............................] - ETA: 1:20 - loss: 0.0464 - accuracy: 0.9859WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 12000 batches). You may need to use the repeat() function when building your dataset.\n",
      "2000/2000 [==============================] - 118s 59ms/step - loss: 0.0464 - accuracy: 0.9859 - val_loss: 0.0722 - val_accuracy: 0.9888\n",
      "INFO:tensorflow:Assets written to: /Users/rijanbhandari/COLLEGE/sixthsem/minorproject/Pahiran/RESTART/model/model_sub/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/rijanbhandari/COLLEGE/sixthsem/minorproject/Pahiran/RESTART/model/model_sub/assets\n",
      "/Users/rijanbhandari/COLLEGE/sixthsem/minorproject/Pahiran/environment/lib/python3.10/site-packages/keras/src/backend.py:5727: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2727/2727 [==============================] - 95s 34ms/step - loss: 0.0536 - accuracy: 0.9870\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.05360173434019089, 0.9869796633720398]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(sub_model)\n",
    "\n",
    "sub_model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "sub_history = sub_model.fit(sub_train, \n",
    "                    epochs=6, \n",
    "                    steps_per_epoch = 2000,\n",
    "                    validation_data = sub_val)\n",
    "\n",
    "sub_model.save(\"/Users/rijanbhandari/COLLEGE/sixthsem/minorproject/Pahiran/RESTART/model/model_sub\")\n",
    "\n",
    "test_model = tf.keras.models.load_model(\"/Users/rijanbhandari/COLLEGE/sixthsem/minorproject/Pahiran/RESTART/model/model_sub\")\n",
    "\n",
    "test_model.evaluate(sub_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "top_df = get_234_df(\"Topwear\")\n",
    "bottom_df = get_234_df(\"Bottomwear\")\n",
    "foot_df = get_234_df(\"Footwear\")\n",
    "top_df,top_art,top_gen,top_base,top_sea,top_usage = my_le(top_df)\n",
    "bottom_df,bottom_art,bottom_gen,bottom_base,bottom_sea,bottom_usage = my_le(bottom_df)\n",
    "foot_df,foot_art,foot_gen,foot_base,foot_sea,foot_usage = my_le(foot_df)\n",
    "foot_usage.classes_\n",
    "\n",
    "top_base_model = build_model(80,60,top_art,top_gen,top_base,top_sea,top_usage)\n",
    "bottom_base_model = build_model(80,60,bottom_art,bottom_gen,bottom_base,bottom_sea,bottom_usage)\n",
    "foot_base_model = build_model(80,60,foot_art,foot_gen,foot_base,foot_sea,foot_usage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_train, top_val, top_test = make_input_xx(make_input_array_2(top_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom_train, bottom_val, bottom_test = make_input_xx(make_input_array_2(bottom_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing image 10284: Failed to load image at path: /Users/rijanbhandari/COLLEGE/sixthsem/minorproject/Pahiran/Miniversion/smalldataset/images/10284.jpg\n"
     ]
    }
   ],
   "source": [
    "foot_train, foot_val, foot_test = make_input_xx(make_input_array_2(foot_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bn/2nh7p1hx30v1yj4_htdlml7c0000gn/T/ipykernel_37265/4125152046.py:2: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
      "  from kerastuner import RandomSearch\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras as keras\n",
    "from kerastuner import RandomSearch\n",
    "\n",
    "def build_model(hp):\n",
    "    # Use the same architecture as your original model\n",
    "    res101 = keras.applications.ResNet101(weights='imagenet', include_top=False, input_shape=(80, 60, 3))\n",
    "    res101.trainable = False\n",
    "    inputs = keras.Input(shape=(80, 60,3),name=\"images\")\n",
    "    x = res101(inputs, training=False)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(hp.Int('dense_units', min_value=32, max_value=512, step=32), activation='relu')(x)\n",
    "    # ... rest of your model architecture ...\n",
    "\n",
    "    model.compile(optimizer=keras.optimizers.Adam(hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])),\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing image 10284: Failed to load image at path: /Users/rijanbhandari/COLLEGE/sixthsem/minorproject/Pahiran/Miniversion/smalldataset/images/10284.jpg\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rijanbhandari/COLLEGE/sixthsem/minorproject/Pahiran/environment/lib/python3.10/site-packages/keras/src/backend.py:5727: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 92s 178ms/step - loss: 6.8284 - articleType_loss: 1.6732 - gender_loss: 0.7560 - baseColour_loss: 2.4970 - season_loss: 0.9465 - usage_loss: 0.9557 - articleType_accuracy: 0.5490 - gender_accuracy: 0.7490 - baseColour_accuracy: 0.2630 - season_accuracy: 0.5530 - usage_accuracy: 0.7250 - val_loss: 5.3851 - val_articleType_loss: 1.1911 - val_gender_loss: 0.4193 - val_baseColour_loss: 2.2345 - val_season_loss: 0.9076 - val_usage_loss: 0.6325 - val_articleType_accuracy: 0.6401 - val_gender_accuracy: 0.8505 - val_baseColour_accuracy: 0.3254 - val_season_accuracy: 0.6164 - val_usage_accuracy: 0.8010\n",
      "Epoch 2/10\n",
      "500/500 [==============================] - 85s 170ms/step - loss: 5.3173 - articleType_loss: 1.2425 - gender_loss: 0.4811 - baseColour_loss: 2.0819 - season_loss: 0.8539 - usage_loss: 0.6578 - articleType_accuracy: 0.6520 - gender_accuracy: 0.8260 - baseColour_accuracy: 0.3360 - season_accuracy: 0.5760 - usage_accuracy: 0.7890 - val_loss: 5.2003 - val_articleType_loss: 1.1454 - val_gender_loss: 0.5156 - val_baseColour_loss: 1.9722 - val_season_loss: 0.8029 - val_usage_loss: 0.7641 - val_articleType_accuracy: 0.6583 - val_gender_accuracy: 0.7988 - val_baseColour_accuracy: 0.3911 - val_season_accuracy: 0.6024 - val_usage_accuracy: 0.7490\n",
      "Epoch 3/10\n",
      "500/500 [==============================] - 1891s 4s/step - loss: 4.8995 - articleType_loss: 1.0870 - gender_loss: 0.4452 - baseColour_loss: 1.9724 - season_loss: 0.8050 - usage_loss: 0.5900 - articleType_accuracy: 0.6810 - gender_accuracy: 0.8530 - baseColour_accuracy: 0.3960 - season_accuracy: 0.5370 - usage_accuracy: 0.8120 - val_loss: 4.6730 - val_articleType_loss: 0.9550 - val_gender_loss: 0.3591 - val_baseColour_loss: 1.8786 - val_season_loss: 0.9702 - val_usage_loss: 0.5102 - val_articleType_accuracy: 0.7289 - val_gender_accuracy: 0.8911 - val_baseColour_accuracy: 0.3839 - val_season_accuracy: 0.5065 - val_usage_accuracy: 0.8264\n",
      "Epoch 4/10\n",
      "500/500 [==============================] - 86s 173ms/step - loss: 4.5548 - articleType_loss: 0.9733 - gender_loss: 0.4323 - baseColour_loss: 1.8236 - season_loss: 0.8053 - usage_loss: 0.5203 - articleType_accuracy: 0.7200 - gender_accuracy: 0.8710 - baseColour_accuracy: 0.4570 - season_accuracy: 0.5670 - usage_accuracy: 0.8230 - val_loss: 4.4366 - val_articleType_loss: 1.1345 - val_gender_loss: 0.2985 - val_baseColour_loss: 1.7342 - val_season_loss: 0.7796 - val_usage_loss: 0.4898 - val_articleType_accuracy: 0.6300 - val_gender_accuracy: 0.9002 - val_baseColour_accuracy: 0.4633 - val_season_accuracy: 0.5647 - val_usage_accuracy: 0.8518\n",
      "Epoch 5/10\n",
      "500/500 [==============================] - 82s 163ms/step - loss: 4.5114 - articleType_loss: 1.0027 - gender_loss: 0.3685 - baseColour_loss: 1.7998 - season_loss: 0.8214 - usage_loss: 0.5189 - articleType_accuracy: 0.7170 - gender_accuracy: 0.8700 - baseColour_accuracy: 0.4380 - season_accuracy: 0.5670 - usage_accuracy: 0.8320 - val_loss: 4.2581 - val_articleType_loss: 0.9251 - val_gender_loss: 0.4040 - val_baseColour_loss: 1.6699 - val_season_loss: 0.8313 - val_usage_loss: 0.4278 - val_articleType_accuracy: 0.7376 - val_gender_accuracy: 0.9008 - val_baseColour_accuracy: 0.4883 - val_season_accuracy: 0.5767 - val_usage_accuracy: 0.8547\n",
      "Epoch 6/10\n",
      "500/500 [==============================] - 79s 159ms/step - loss: 4.1574 - articleType_loss: 0.8772 - gender_loss: 0.3406 - baseColour_loss: 1.6709 - season_loss: 0.7809 - usage_loss: 0.4878 - articleType_accuracy: 0.7450 - gender_accuracy: 0.8880 - baseColour_accuracy: 0.4790 - season_accuracy: 0.6080 - usage_accuracy: 0.8490 - val_loss: 4.4318 - val_articleType_loss: 0.8934 - val_gender_loss: 0.3440 - val_baseColour_loss: 1.8016 - val_season_loss: 0.7817 - val_usage_loss: 0.6111 - val_articleType_accuracy: 0.7071 - val_gender_accuracy: 0.8739 - val_baseColour_accuracy: 0.4841 - val_season_accuracy: 0.6320 - val_usage_accuracy: 0.8244\n",
      "Epoch 7/10\n",
      "500/500 [==============================] - 86s 172ms/step - loss: 4.1666 - articleType_loss: 0.8849 - gender_loss: 0.2839 - baseColour_loss: 1.6166 - season_loss: 0.8605 - usage_loss: 0.5207 - articleType_accuracy: 0.7250 - gender_accuracy: 0.9140 - baseColour_accuracy: 0.5080 - season_accuracy: 0.6330 - usage_accuracy: 0.8380 - val_loss: 4.1229 - val_articleType_loss: 0.9343 - val_gender_loss: 0.2760 - val_baseColour_loss: 1.7030 - val_season_loss: 0.8010 - val_usage_loss: 0.4087 - val_articleType_accuracy: 0.7559 - val_gender_accuracy: 0.9077 - val_baseColour_accuracy: 0.4889 - val_season_accuracy: 0.5660 - val_usage_accuracy: 0.8648\n",
      "Epoch 8/10\n",
      "500/500 [==============================] - 86s 171ms/step - loss: 4.0386 - articleType_loss: 0.9074 - gender_loss: 0.3338 - baseColour_loss: 1.6050 - season_loss: 0.7651 - usage_loss: 0.4273 - articleType_accuracy: 0.7340 - gender_accuracy: 0.8900 - baseColour_accuracy: 0.5160 - season_accuracy: 0.6030 - usage_accuracy: 0.8770 - val_loss: 3.8386 - val_articleType_loss: 0.8457 - val_gender_loss: 0.2883 - val_baseColour_loss: 1.5151 - val_season_loss: 0.7385 - val_usage_loss: 0.4511 - val_articleType_accuracy: 0.7568 - val_gender_accuracy: 0.8963 - val_baseColour_accuracy: 0.5377 - val_season_accuracy: 0.6232 - val_usage_accuracy: 0.8534\n",
      "Epoch 9/10\n",
      "500/500 [==============================] - 81s 161ms/step - loss: 4.0001 - articleType_loss: 0.8591 - gender_loss: 0.2943 - baseColour_loss: 1.5836 - season_loss: 0.7535 - usage_loss: 0.5095 - articleType_accuracy: 0.7550 - gender_accuracy: 0.9120 - baseColour_accuracy: 0.5300 - season_accuracy: 0.6290 - usage_accuracy: 0.8410 - val_loss: 3.8341 - val_articleType_loss: 0.8216 - val_gender_loss: 0.2590 - val_baseColour_loss: 1.5189 - val_season_loss: 0.7977 - val_usage_loss: 0.4367 - val_articleType_accuracy: 0.7435 - val_gender_accuracy: 0.9119 - val_baseColour_accuracy: 0.5283 - val_season_accuracy: 0.4915 - val_usage_accuracy: 0.8462\n",
      "Epoch 10/10\n",
      "115/500 [=====>........................] - ETA: 21s - loss: 3.6815 - articleType_loss: 0.7580 - gender_loss: 0.2398 - baseColour_loss: 1.6019 - season_loss: 0.7202 - usage_loss: 0.3617 - articleType_accuracy: 0.7773 - gender_accuracy: 0.9214 - baseColour_accuracy: 0.4716 - season_accuracy: 0.6463 - usage_accuracy: 0.8646WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 5000 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 5000 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 64s 128ms/step - loss: 3.6815 - articleType_loss: 0.7580 - gender_loss: 0.2398 - baseColour_loss: 1.6019 - season_loss: 0.7202 - usage_loss: 0.3617 - articleType_accuracy: 0.7773 - gender_accuracy: 0.9214 - baseColour_accuracy: 0.4716 - season_accuracy: 0.6463 - usage_accuracy: 0.8646 - val_loss: 3.8148 - val_articleType_loss: 0.7953 - val_gender_loss: 0.2832 - val_baseColour_loss: 1.4004 - val_season_loss: 0.8447 - val_usage_loss: 0.4912 - val_articleType_accuracy: 0.7435 - val_gender_accuracy: 0.9109 - val_baseColour_accuracy: 0.5637 - val_season_accuracy: 0.6255 - val_usage_accuracy: 0.8560\n",
      "1539/1539 [==============================] - 56s 36ms/step - loss: 3.8095 - articleType_loss: 0.8109 - gender_loss: 0.2495 - baseColour_loss: 1.4135 - season_loss: 0.8439 - usage_loss: 0.4916 - articleType_accuracy: 0.7427 - gender_accuracy: 0.9159 - baseColour_accuracy: 0.5552 - season_accuracy: 0.6264 - usage_accuracy: 0.8535\n",
      "INFO:tensorflow:Assets written to: /Users/rijanbhandari/COLLEGE/sixthsem/minorproject/Pahiran/RESTART/model/model_top/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/rijanbhandari/COLLEGE/sixthsem/minorproject/Pahiran/RESTART/model/model_top/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rijanbhandari/COLLEGE/sixthsem/minorproject/Pahiran/environment/lib/python3.10/site-packages/keras/src/backend.py:5727: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 18s 255ms/step - loss: 13.3958 - articleType_loss: 3.3855 - gender_loss: 1.6340 - baseColour_loss: 3.5364 - season_loss: 2.3609 - usage_loss: 2.4790 - articleType_accuracy: 0.3200 - gender_accuracy: 0.5900 - baseColour_accuracy: 0.3200 - season_accuracy: 0.4900 - usage_accuracy: 0.6700 - val_loss: 7.4600 - val_articleType_loss: 2.0074 - val_gender_loss: 0.9756 - val_baseColour_loss: 2.2470 - val_season_loss: 1.2600 - val_usage_loss: 0.9700 - val_articleType_accuracy: 0.3631 - val_gender_accuracy: 0.6816 - val_baseColour_accuracy: 0.2868 - val_season_accuracy: 0.4339 - val_usage_accuracy: 0.6518\n",
      "Epoch 2/15\n",
      "50/50 [==============================] - 13s 256ms/step - loss: 7.2664 - articleType_loss: 1.8412 - gender_loss: 0.9338 - baseColour_loss: 2.0861 - season_loss: 1.2096 - usage_loss: 1.1957 - articleType_accuracy: 0.3900 - gender_accuracy: 0.6500 - baseColour_accuracy: 0.4100 - season_accuracy: 0.4800 - usage_accuracy: 0.5500 - val_loss: 7.5881 - val_articleType_loss: 2.0275 - val_gender_loss: 1.1185 - val_baseColour_loss: 2.0632 - val_season_loss: 1.0386 - val_usage_loss: 1.3404 - val_articleType_accuracy: 0.4283 - val_gender_accuracy: 0.5717 - val_baseColour_accuracy: 0.3482 - val_season_accuracy: 0.4823 - val_usage_accuracy: 0.6034\n",
      "Epoch 3/15\n",
      "50/50 [==============================] - 13s 253ms/step - loss: 7.2923 - articleType_loss: 2.1122 - gender_loss: 1.0200 - baseColour_loss: 2.2837 - season_loss: 0.8762 - usage_loss: 1.0002 - articleType_accuracy: 0.3900 - gender_accuracy: 0.5800 - baseColour_accuracy: 0.2900 - season_accuracy: 0.5700 - usage_accuracy: 0.6200 - val_loss: 7.0961 - val_articleType_loss: 2.0507 - val_gender_loss: 0.8953 - val_baseColour_loss: 2.1592 - val_season_loss: 1.1048 - val_usage_loss: 0.8861 - val_articleType_accuracy: 0.3222 - val_gender_accuracy: 0.6276 - val_baseColour_accuracy: 0.2477 - val_season_accuracy: 0.4358 - val_usage_accuracy: 0.6872\n",
      "Epoch 4/15\n",
      "50/50 [==============================] - 12s 236ms/step - loss: 6.9095 - articleType_loss: 2.0574 - gender_loss: 0.8108 - baseColour_loss: 2.1644 - season_loss: 1.0356 - usage_loss: 0.8413 - articleType_accuracy: 0.3600 - gender_accuracy: 0.6300 - baseColour_accuracy: 0.3200 - season_accuracy: 0.5600 - usage_accuracy: 0.7000 - val_loss: 6.3457 - val_articleType_loss: 1.7607 - val_gender_loss: 0.6552 - val_baseColour_loss: 2.1557 - val_season_loss: 0.8027 - val_usage_loss: 0.9714 - val_articleType_accuracy: 0.4544 - val_gender_accuracy: 0.7281 - val_baseColour_accuracy: 0.3222 - val_season_accuracy: 0.6760 - val_usage_accuracy: 0.7225\n",
      "Epoch 5/15\n",
      "50/50 [==============================] - 12s 233ms/step - loss: 6.3664 - articleType_loss: 1.7465 - gender_loss: 0.7229 - baseColour_loss: 2.0653 - season_loss: 0.8598 - usage_loss: 0.9719 - articleType_accuracy: 0.3800 - gender_accuracy: 0.6500 - baseColour_accuracy: 0.3700 - season_accuracy: 0.6600 - usage_accuracy: 0.6900 - val_loss: 6.0640 - val_articleType_loss: 1.5859 - val_gender_loss: 0.8790 - val_baseColour_loss: 1.8688 - val_season_loss: 0.9207 - val_usage_loss: 0.8096 - val_articleType_accuracy: 0.5382 - val_gender_accuracy: 0.6611 - val_baseColour_accuracy: 0.3911 - val_season_accuracy: 0.5400 - val_usage_accuracy: 0.6853\n",
      "Epoch 6/15\n",
      "50/50 [==============================] - 11s 228ms/step - loss: 6.4060 - articleType_loss: 1.7604 - gender_loss: 0.8244 - baseColour_loss: 2.0780 - season_loss: 0.9066 - usage_loss: 0.8367 - articleType_accuracy: 0.4400 - gender_accuracy: 0.6500 - baseColour_accuracy: 0.3000 - season_accuracy: 0.5900 - usage_accuracy: 0.7000 - val_loss: 6.1398 - val_articleType_loss: 1.5806 - val_gender_loss: 0.7282 - val_baseColour_loss: 1.9309 - val_season_loss: 0.9949 - val_usage_loss: 0.9052 - val_articleType_accuracy: 0.4730 - val_gender_accuracy: 0.6425 - val_baseColour_accuracy: 0.3818 - val_season_accuracy: 0.5568 - val_usage_accuracy: 0.6797\n",
      "Epoch 7/15\n",
      "50/50 [==============================] - 12s 241ms/step - loss: 5.9927 - articleType_loss: 1.6356 - gender_loss: 0.7851 - baseColour_loss: 1.8421 - season_loss: 0.8225 - usage_loss: 0.9074 - articleType_accuracy: 0.4800 - gender_accuracy: 0.7100 - baseColour_accuracy: 0.3000 - season_accuracy: 0.6600 - usage_accuracy: 0.6600 - val_loss: 6.1288 - val_articleType_loss: 1.7623 - val_gender_loss: 0.6866 - val_baseColour_loss: 2.0862 - val_season_loss: 0.8532 - val_usage_loss: 0.7405 - val_articleType_accuracy: 0.4618 - val_gender_accuracy: 0.7300 - val_baseColour_accuracy: 0.2868 - val_season_accuracy: 0.6238 - val_usage_accuracy: 0.7244\n",
      "Epoch 8/15\n",
      "50/50 [==============================] - 13s 269ms/step - loss: 6.6221 - articleType_loss: 1.7599 - gender_loss: 0.8532 - baseColour_loss: 1.9166 - season_loss: 1.1530 - usage_loss: 0.9394 - articleType_accuracy: 0.4500 - gender_accuracy: 0.7000 - baseColour_accuracy: 0.4200 - season_accuracy: 0.6000 - usage_accuracy: 0.6000 - val_loss: 5.5043 - val_articleType_loss: 1.5502 - val_gender_loss: 0.4776 - val_baseColour_loss: 1.9114 - val_season_loss: 0.8675 - val_usage_loss: 0.6975 - val_articleType_accuracy: 0.4823 - val_gender_accuracy: 0.8156 - val_baseColour_accuracy: 0.3650 - val_season_accuracy: 0.5345 - val_usage_accuracy: 0.6983\n",
      "Epoch 9/15\n",
      "50/50 [==============================] - 13s 271ms/step - loss: 5.7353 - articleType_loss: 1.5309 - gender_loss: 0.5981 - baseColour_loss: 1.8507 - season_loss: 0.9237 - usage_loss: 0.8318 - articleType_accuracy: 0.4600 - gender_accuracy: 0.7200 - baseColour_accuracy: 0.4300 - season_accuracy: 0.5900 - usage_accuracy: 0.6600 - val_loss: 5.4085 - val_articleType_loss: 1.4206 - val_gender_loss: 0.6017 - val_baseColour_loss: 1.7457 - val_season_loss: 0.8384 - val_usage_loss: 0.8021 - val_articleType_accuracy: 0.5456 - val_gender_accuracy: 0.7709 - val_baseColour_accuracy: 0.4078 - val_season_accuracy: 0.6294 - val_usage_accuracy: 0.6834\n",
      "Epoch 10/15\n",
      "50/50 [==============================] - 12s 247ms/step - loss: 6.2547 - articleType_loss: 1.7516 - gender_loss: 0.7604 - baseColour_loss: 2.0272 - season_loss: 0.7941 - usage_loss: 0.9214 - articleType_accuracy: 0.3700 - gender_accuracy: 0.7000 - baseColour_accuracy: 0.3400 - season_accuracy: 0.6000 - usage_accuracy: 0.6200 - val_loss: 5.9199 - val_articleType_loss: 1.5272 - val_gender_loss: 0.6774 - val_baseColour_loss: 1.8936 - val_season_loss: 0.9413 - val_usage_loss: 0.8805 - val_articleType_accuracy: 0.5009 - val_gender_accuracy: 0.7542 - val_baseColour_accuracy: 0.3371 - val_season_accuracy: 0.5959 - val_usage_accuracy: 0.6909\n",
      "Epoch 11/15\n",
      "50/50 [==============================] - 12s 249ms/step - loss: 6.1134 - articleType_loss: 1.5979 - gender_loss: 0.7170 - baseColour_loss: 1.8864 - season_loss: 1.0020 - usage_loss: 0.9100 - articleType_accuracy: 0.5500 - gender_accuracy: 0.7500 - baseColour_accuracy: 0.4200 - season_accuracy: 0.5600 - usage_accuracy: 0.7300 - val_loss: 6.0076 - val_articleType_loss: 1.4612 - val_gender_loss: 0.8028 - val_baseColour_loss: 1.9913 - val_season_loss: 0.8791 - val_usage_loss: 0.8732 - val_articleType_accuracy: 0.4953 - val_gender_accuracy: 0.6778 - val_baseColour_accuracy: 0.3147 - val_season_accuracy: 0.5307 - val_usage_accuracy: 0.6574\n",
      "Epoch 12/15\n",
      "50/50 [==============================] - 13s 264ms/step - loss: 6.0143 - articleType_loss: 1.5978 - gender_loss: 0.7209 - baseColour_loss: 1.9682 - season_loss: 0.9947 - usage_loss: 0.7327 - articleType_accuracy: 0.5100 - gender_accuracy: 0.7300 - baseColour_accuracy: 0.3100 - season_accuracy: 0.5700 - usage_accuracy: 0.7600 - val_loss: 5.6855 - val_articleType_loss: 1.4760 - val_gender_loss: 0.5977 - val_baseColour_loss: 1.9090 - val_season_loss: 0.8441 - val_usage_loss: 0.8586 - val_articleType_accuracy: 0.4953 - val_gender_accuracy: 0.7635 - val_baseColour_accuracy: 0.3203 - val_season_accuracy: 0.6294 - val_usage_accuracy: 0.7132\n",
      "Epoch 13/15\n",
      "50/50 [==============================] - 13s 270ms/step - loss: 5.8497 - articleType_loss: 1.6857 - gender_loss: 0.6624 - baseColour_loss: 1.8470 - season_loss: 0.8941 - usage_loss: 0.7605 - articleType_accuracy: 0.4100 - gender_accuracy: 0.7700 - baseColour_accuracy: 0.3300 - season_accuracy: 0.5700 - usage_accuracy: 0.6700 - val_loss: 5.3015 - val_articleType_loss: 1.3105 - val_gender_loss: 0.4898 - val_baseColour_loss: 1.8572 - val_season_loss: 0.7997 - val_usage_loss: 0.8443 - val_articleType_accuracy: 0.5624 - val_gender_accuracy: 0.7970 - val_baseColour_accuracy: 0.3333 - val_season_accuracy: 0.6574 - val_usage_accuracy: 0.6927\n",
      "Epoch 14/15\n",
      "50/50 [==============================] - 14s 276ms/step - loss: 5.8475 - articleType_loss: 1.6482 - gender_loss: 0.5956 - baseColour_loss: 1.9384 - season_loss: 0.9414 - usage_loss: 0.7240 - articleType_accuracy: 0.5300 - gender_accuracy: 0.7400 - baseColour_accuracy: 0.4200 - season_accuracy: 0.5800 - usage_accuracy: 0.7000 - val_loss: 5.2912 - val_articleType_loss: 1.3040 - val_gender_loss: 0.6236 - val_baseColour_loss: 1.7039 - val_season_loss: 0.8845 - val_usage_loss: 0.7751 - val_articleType_accuracy: 0.5531 - val_gender_accuracy: 0.7803 - val_baseColour_accuracy: 0.4525 - val_season_accuracy: 0.5196 - val_usage_accuracy: 0.6983\n",
      "Epoch 15/15\n",
      "50/50 [==============================] - 12s 248ms/step - loss: 5.8340 - articleType_loss: 1.6361 - gender_loss: 0.6211 - baseColour_loss: 1.8066 - season_loss: 1.0517 - usage_loss: 0.7185 - articleType_accuracy: 0.4600 - gender_accuracy: 0.7300 - baseColour_accuracy: 0.3800 - season_accuracy: 0.5000 - usage_accuracy: 0.8100 - val_loss: 5.2544 - val_articleType_loss: 1.4276 - val_gender_loss: 0.4863 - val_baseColour_loss: 1.7699 - val_season_loss: 0.8823 - val_usage_loss: 0.6882 - val_articleType_accuracy: 0.5680 - val_gender_accuracy: 0.7933 - val_baseColour_accuracy: 0.3929 - val_season_accuracy: 0.6034 - val_usage_accuracy: 0.7430\n",
      "269/269 [==============================] - 10s 36ms/step - loss: 4.9525 - articleType_loss: 1.2785 - gender_loss: 0.4639 - baseColour_loss: 1.6589 - season_loss: 0.8316 - usage_loss: 0.7196 - articleType_accuracy: 0.5959 - gender_accuracy: 0.8156 - baseColour_accuracy: 0.4246 - season_accuracy: 0.6220 - usage_accuracy: 0.7542\n",
      "INFO:tensorflow:Assets written to: /Users/rijanbhandari/COLLEGE/sixthsem/minorproject/Pahiran/RESTART/model/model_bottom/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/rijanbhandari/COLLEGE/sixthsem/minorproject/Pahiran/RESTART/model/model_bottom/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rijanbhandari/COLLEGE/sixthsem/minorproject/Pahiran/environment/lib/python3.10/site-packages/keras/src/backend.py:5727: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 149s 73ms/step - loss: 5.5100 - articleType_loss: 1.2641 - gender_loss: 0.6966 - baseColour_loss: 1.7360 - season_loss: 1.1766 - usage_loss: 0.6366 - articleType_accuracy: 0.5533 - gender_accuracy: 0.7607 - baseColour_accuracy: 0.4750 - season_accuracy: 0.4895 - usage_accuracy: 0.7623 - val_loss: 4.7162 - val_articleType_loss: 0.9184 - val_gender_loss: 0.6791 - val_baseColour_loss: 1.5007 - val_season_loss: 1.1070 - val_usage_loss: 0.5110 - val_articleType_accuracy: 0.6476 - val_gender_accuracy: 0.7450 - val_baseColour_accuracy: 0.5623 - val_season_accuracy: 0.5057 - val_usage_accuracy: 0.8189\n",
      "Epoch 2/5\n",
      " 759/2000 [==========>...................] - ETA: 1:04 - loss: 4.8163 - articleType_loss: 1.0319 - gender_loss: 0.6677 - baseColour_loss: 1.4975 - season_loss: 1.0963 - usage_loss: 0.5230 - articleType_accuracy: 0.6383 - gender_accuracy: 0.7688 - baseColour_accuracy: 0.5626 - season_accuracy: 0.5184 - usage_accuracy: 0.7912WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 72s 36ms/step - loss: 4.8163 - articleType_loss: 1.0319 - gender_loss: 0.6677 - baseColour_loss: 1.4975 - season_loss: 1.0963 - usage_loss: 0.5230 - articleType_accuracy: 0.6383 - gender_accuracy: 0.7688 - baseColour_accuracy: 0.5626 - season_accuracy: 0.5184 - usage_accuracy: 0.7912 - val_loss: 4.3709 - val_articleType_loss: 0.8382 - val_gender_loss: 0.6539 - val_baseColour_loss: 1.3934 - val_season_loss: 1.0561 - val_usage_loss: 0.4294 - val_articleType_accuracy: 0.6939 - val_gender_accuracy: 0.8075 - val_baseColour_accuracy: 0.5742 - val_season_accuracy: 0.5155 - val_usage_accuracy: 0.8439\n",
      "920/920 [==============================] - 34s 37ms/step - loss: 4.3960 - articleType_loss: 0.8345 - gender_loss: 0.6540 - baseColour_loss: 1.3998 - season_loss: 1.0594 - usage_loss: 0.4483 - articleType_accuracy: 0.6810 - gender_accuracy: 0.8120 - baseColour_accuracy: 0.5935 - season_accuracy: 0.5223 - usage_accuracy: 0.8386\n",
      "INFO:tensorflow:Assets written to: /Users/rijanbhandari/COLLEGE/sixthsem/minorproject/Pahiran/RESTART/model/model_foot/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/rijanbhandari/COLLEGE/sixthsem/minorproject/Pahiran/RESTART/model/model_foot/assets\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "foot_train, foot_val, foot_test = make_input_xx(make_input_array_2(foot_df))\n",
    "\n",
    "top_base_model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "bottom_base_model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "foot_base_model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "top_history = top_base_model.fit(top_train, \n",
    "                    epochs=10, \n",
    "                    steps_per_epoch = 500,\n",
    "                    validation_data = top_val)\n",
    "\n",
    "top_base_model.evaluate(top_test)\n",
    "\n",
    "top_base_model.save(\"/Users/rijanbhandari/COLLEGE/sixthsem/minorproject/Pahiran/RESTART/model/model_top\")\n",
    "\n",
    "\n",
    "\n",
    "bottom_history = bottom_base_model.fit(bottom_train, \n",
    "                    epochs=15, \n",
    "                    steps_per_epoch = 50,\n",
    "                    validation_data = bottom_val)\n",
    "\n",
    "bottom_base_model.evaluate(bottom_test)\n",
    "\n",
    "bottom_base_model.save(\"/Users/rijanbhandari/COLLEGE/sixthsem/minorproject/Pahiran/RESTART/model/model_bottom\")\n",
    "\n",
    "\n",
    "\n",
    "foot_history = foot_base_model.fit(foot_train, \n",
    "                    epochs=5, \n",
    "                    steps_per_epoch = 2000,\n",
    "                    validation_data = foot_val)\n",
    "\n",
    "foot_base_model.evaluate(foot_test)\n",
    "\n",
    "foot_base_model.save(\"/Users/rijanbhandari/COLLEGE/sixthsem/minorproject/Pahiran/RESTART/model/model_foot\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
